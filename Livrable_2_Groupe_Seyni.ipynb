{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Projet HumanForYou : Prédiction du Turnover par Intelligence Artificielle**\n",
        "\n",
        "## Livrable 2 - Notebook Jupyter\n",
        "\n",
        "<!-- Image/Bannière de présentation -->\n",
        "![Bannière du projet HumanForYou](../docs/imgs/projectbanner.png)\n",
        "<!-- Image/Bannière de présentation -->\n",
        "---\n",
        "\n",
        "**École** : CESI École d'Ingénieurs  \n",
        "**Programme** : A3 FISE INFO - Bloc Intelligence Artificielle (Machine Learning)  \n",
        "**Date de réalisation** : Décembre 2025  \n",
        "**Date de livraison** : 18 décembre 2025\n",
        "\n",
        "---\n",
        "\n",
        "### Équipe Projet - Groupe 5 (CESI Analytics)\n",
        "\n",
        "| Rôle | Membre | Responsabilités |\n",
        "|------|--------|------------------|\n",
        "| Data Engineer | ODDOUX Diego | Fusion des données, modèle Random Forest |\n",
        "| Data Analyst | EL GHAZAL Omar | EDA, audit éthique, évaluation |\n",
        "| ML Engineer | KING Mickaël | Préprocessing, documentation |\n",
        "| Data Scientist | BALDE Seyni Junior | Régression Logistique, consolidation finale |\n",
        "\n",
        "**Encadrant pédagogique** : Mme. GENANE Youness\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PHASE 1 : CADRAGE ET ÉTUDE DES DONNÉES\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction et Contexte\n",
        "\n",
        "### 1.1 Présentation de HumanForYou\n",
        "\n",
        "HumanForYou est une entreprise majeure du secteur pharmaceutique basée en Inde. Avec un effectif d'environ **4 000 collaborateurs**, elle opère dans un environnement hautement concurrentiel où la rétention des talents et la stabilité des équipes constituent des vecteurs essentiels de la performance industrielle et de la recherche et développement.\n",
        "\n",
        "L'organisation se concentre sur la production de médicaments de qualité supérieure et le maintien d'une excellence opérationnelle. Cependant, la gestion efficace des ressources humaines représente un défi majeur dans ce contexte de forte compétitivité.\n",
        "\n",
        "### 1.2 Le problème du turnover\n",
        "\n",
        "L'entreprise fait face à un défi structurel majeur : un **taux de rotation annuel (turnover) avoisinant les $15\\%$**. Concrètement, cela représente environ **600 départs par an** parmi les 4 000 employés. Ce taux est jugé critique par la direction car il dépasse les standards du secteur pharmaceutique (estimés entre 8 % et 12 %) et fragilise la pérennité des opérations.\n",
        "\n",
        "### 1.3 Enjeux métier et impact du projet\n",
        "\n",
        "Ce niveau de rotation engendre des conséquences néfastes multidimensionnelles pour l'organisation :\n",
        "\n",
        "**1. Coûts directs**\n",
        "- Recrutement externe et processus de sélection\n",
        "- Intégration et formation exhaustive des nouveaux entrants\n",
        "- Estimation : chaque départ représente un coût minimal de **50 000 euros** (salaire annuel moyen, formation, logistique)\n",
        "- **Coût total annuel** : environ **30 millions d'euros**\n",
        "\n",
        "**2. Coûts indirects**\n",
        "- Perte de savoir-faire spécialisé (critique dans les secteurs R&D et qualité)\n",
        "- Ralentissement des projets en cours due aux lacunes temporaires en effectifs\n",
        "- Baisse de la productivité durant les phases de transition et de montée en compétences\n",
        "- Dégradation de la qualité des livrables\n",
        "\n",
        "**3. Impact réputationnel**\n",
        "- Risque de dégradation de la marque employeur\n",
        "- Difficultés accrues dans le recrutement de profils de haut niveau\n",
        "- Instabilité perçue par les clients et partenaires commerciaux\n",
        "\n",
        "**Enjeu stratégique du projet** : Passer d'une gestion **réactive** des ressources humaines à une gestion **proactive**, en exploitant les données historiques (2015) pour anticiper les départs volontaires et proposer des leviers d'action RH ciblés, mesurables et éthiquement responsables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Problématique et Objectifs\n",
        "\n",
        "### 2.1 Problématique centrale\n",
        "\n",
        "**Comment identifier de manière fiable et éthique les facteurs prédictifs du départ des employés afin de proposer des leviers d'action RH ciblés, tout en garantissant une démarche respectueuse de la vie privée et conforme aux principes d'équité et de non-discrimination énoncés par l'AI Act 2024 ?**\n",
        "\n",
        "Cette problématique centrale soulève plusieurs dimensions interdépendantes :\n",
        "\n",
        "- **Dimension technique** : Construire des modèles prédictifs robustes et généralisables.\n",
        "- **Dimension éthique** : Garantir la transparence et l'équité du système décisionnel.\n",
        "- **Dimension métier** : Traduire les prédictions en actions concrètes et mesurables pour les équipes RH.\n",
        "\n",
        "### 2.2 Objectifs techniques\n",
        "\n",
        "Pour répondre à cette problématique, nous mettrons en œuvre et comparerons deux approches de classification supervisée :\n",
        "\n",
        "**1. Régression Logistique**\n",
        "- Modèle linéaire de probabilité : $P(\\text{Attrition} = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\sum_{i=1}^{n} \\beta_i X_i)}}$\n",
        "- Interprétabilité maximale : chaque coefficient $\\beta_i$ représente l'impact d'une variable sur la probabilité de départ.\n",
        "- Transparence : explicabilité directe vis-à-vis des parties prenantes (RH, direction).\n",
        "- Performance attendue : bonne baseline pour classification.\n",
        "\n",
        "**2. Random Forest**\n",
        "- Ensemble d'arbres de décision avec bagging et sous-échantillonnage des features.\n",
        "- Capacité à capturer des non-linéarités et interactions complexes.\n",
        "- Robustesse : moins sensible aux données aberrantes et aux valeurs manquantes.\n",
        "- Feature importance : identification hiérarchisée des facteurs critiques.\n",
        "\n",
        "**Métriques d'évaluation** :\n",
        "- **F1-Score** : Équilibre entre précision et rappel (prioritaire si classes déséquilibrées)\n",
        "- **AUC-ROC** : Capacité discriminante du modèle indépendamment du seuil de classification\n",
        "- **Précision et Rappel** : Analyse détaillée des types d'erreurs (faux positifs vs faux négatifs)\n",
        "- **Matrice de confusion** : Distribution des prédictions par classe\n",
        "- **Validation croisée 5-fold** : Assurer la stabilité et la généralisation\n",
        "\n",
        "### 2.3 Objectifs éthiques (Conformité AI Act 2024)\n",
        "\n",
        "Conformément au **Livrable 1 (Rapport IA et Éthique)**, ce projet s'inscrit dans le respect strict des **7 exigences de l'Union Européenne** pour une Intelligence Artificielle digne de confiance :\n",
        "\n",
        "| Exigence | Application au Projet |\n",
        "|----------|----------------------|\n",
        "| **1. Autonomie humaine** | Outil d'aide à la décision, pas de décision automatisée. Les équipes RH conservent le pouvoir décisionnel final. |\n",
        "| **2. Robustesse et sécurité** | Validation croisée 5-fold, détection d'anomalies, stabilité des résultats. |\n",
        "| **3. Confidentialité et gouvernance** | Pseudonymisation des données, limitation des finalités (usage RH uniquement), respect du RGPD. |\n",
        "| **4. Transparence** | Explicabilité des coefficients (LR) et importance des variables (RF). |\n",
        "| **5. Non-discrimination et équité** | Exclusion variables protégées (Age, Gender, MaritalStatus). Surveillance variables proxy. |\n",
        "| **6. Bien-être environnemental et sociétal** | Objectif : amélioration conditions de travail, pas surveillance. |\n",
        "| **7. Responsabilité** | Rôles définis, traçabilité, audit trail des décisions. |\n",
        "\n",
        "### 2.4 Vue d'ensemble de la démarche\n",
        "\n",
        "Notre méthodologie suit une approche **itérative et rigoureuse** structurée en **5 phases principales** :\n",
        "\n",
        "```\n",
        "╔═══════════════════════════════════════════════════════════╗\n",
        "║  PHASE 1 : Exploration et Audit (Parties 1-3)            ║\n",
        "║  ├─ Fusion des données (3 fichiers RH + 2 fichiers temp)║\n",
        "║  ├─ Analyse exploratoire (EDA)                           ║\n",
        "║  └─ Audit éthique des variables                          ║\n",
        "╠═══════════════════════════════════════════════════════════╣\n",
        "║  PHASE 2 : Préparation (Partie 4)                        ║\n",
        "║  ├─ Gestion des valeurs manquantes                       ║\n",
        "║  ├─ Feature engineering                                  ║\n",
        "║  ├─ Encodage et standardisation                          ║\n",
        "║  └─ Split train/test stratifié                           ║\n",
        "╠═══════════════════════════════════════════════════════════╣\n",
        "║  PHASE 3 : Modélisation (Parties 5-6)                   ║\n",
        "║  ├─ Régression Logistique                                ║\n",
        "║  ├─ Random Forest                                        ║\n",
        "║  └─ Optimisation hyperparamètres                         ║\n",
        "╠═══════════════════════════════════════════════════════════╣\n",
        "║  PHASE 4 : Évaluation et Comparaison (Partie 7)          ║\n",
        "║  ├─ Métriques de performance                             ║\n",
        "║  ├─ Analyse équité                                       ║\n",
        "║  └─ Choix du modèle final                                ║\n",
        "╠═══════════════════════════════════════════════════════════╣\n",
        "║  PHASE 5 : Recommandations et Insights (Partie 8)        ║\n",
        "║  ├─ Facteurs critiques de départ                         ║\n",
        "║  ├─ Leviers d'action RH                                  ║\n",
        "║  └─ Roadmap de mise en production                        ║\n",
        "╚═══════════════════════════════════════════════════════════╝\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Présentation des Jeux de Données\n",
        "\n",
        "Les données fournies par le service des Ressources Humaines se composent de **5 fichiers distincts**, couvrant l'année d'exercice **2015** comme base d'observation pour prédire les départs en 2016.\n",
        "\n",
        "### 3.1 Description détaillée des fichiers sources\n",
        "\n",
        "Nous distinguons deux catégories de fichiers selon leur granularité et leur structure :\n",
        "\n",
        "#### 3.1.1 `general_data.csv` - Données RH principales\n",
        "\n",
        "**Caractéristiques** :\n",
        "- **Grain** : 1 ligne = 1 employé\n",
        "- **Nombre de variables** : 35\n",
        "- **Taille approximative** : 550 KB\n",
        "- **Contenu** : Informations contractuelles, salariales, démographiques et d'organisation\n",
        "\n",
        "**Variables clés** :\n",
        "- **Identifiant** : `EmployeeID`\n",
        "- **Démographiques** : Age, Gender, MaritalStatus\n",
        "- **Organisationnelles** : Department, JobRole, JobLevel (1-5)\n",
        "- **Contractuelles** : MonthlyIncome, YearsAtCompany, TotalWorkingYears\n",
        "- **Professionnelles** : TrainingTimesLastYear, YearsSinceLastPromotion, PercentSalaryHike\n",
        "- **Comportementales** : OverTime (Yes/No), BusinessTravel, DistanceFromHome\n",
        "- **Cible** : **Attrition** (0 = resté en 2016, 1 = parti en 2016)\n",
        "\n",
        "#### 3.1.2 `manager_survey_data.csv` - Évaluations annuelles\n",
        "\n",
        "**Caractéristiques** :\n",
        "- **Grain** : 1 ligne = 1 employé\n",
        "- **Nombre de variables** : 3 (EmployeeID + 2 métriques)\n",
        "- **Taille approximative** : 43 KB\n",
        "- **Contenu** : Évaluations fournies par le manager direct en février 2015\n",
        "\n",
        "**Variables** :\n",
        "- `EmployeeID` : Identifiant unique (clé de fusion)\n",
        "- `JobInvolvement` : Implication au travail (1 = Faible, 2 = Moyen, 3 = Élevé, 4 = Très élevé)\n",
        "- `PerformanceRating` : Performance annuelle (1 = Faible, 2 = Bon, 3 = Excellent, 4 = Au-delà des attentes)\n",
        "\n",
        "#### 3.1.3 `employee_survey_data.csv` - Enquête qualité de vie au travail\n",
        "\n",
        "**Caractéristiques** :\n",
        "- **Grain** : 1 ligne = 1 employé\n",
        "- **Nombre de variables** : 4 (EmployeeID + 3 métriques)\n",
        "- **Taille approximative** : 52 KB\n",
        "- **Contenu** : Résultats d'une enquête volontaire soumise en juin 2015\n",
        "- **Particularité** : Enquête **non obligatoire** → présence de valeurs manquantes (NA)\n",
        "\n",
        "**Variables** :\n",
        "- `EmployeeID` : Identifiant unique (clé de fusion)\n",
        "- `EnvironmentSatisfaction` : Satisfaction environnement de travail (1 = Faible, 2 = Moyen, 3 = Élevé, 4 = Très élevé)\n",
        "- `JobSatisfaction` : Satisfaction du travail (1-4, même échelle)\n",
        "- `WorkLifeBalance` : Équilibre vie professionnelle/personnelle (1 = Mauvais, 2 = Satisfaisant, 3 = Très satisfaisant, 4 = Excellent)\n",
        "\n",
        "#### 3.1.4 `in_time.csv` - Horaires d'arrivée (Badgeuse)\n",
        "\n",
        "**Caractéristiques** :\n",
        "- **Grain** : 1 ligne = 1 employé\n",
        "- **Structure** : 1 colonne par date (365 jours de 2015)\n",
        "- **Taille approximative** : Variable (données compressées en .zip)\n",
        "- **Contenu** : Heures d'arrivée quotidiennes (pointage badgeuse) ou NA (jour non travaillé)\n",
        "- **Format temporel** : Très granularisé (nécessite agrégation)\n",
        "\n",
        "#### 3.1.5 `out_time.csv` - Horaires de départ (Badgeuse)\n",
        "\n",
        "**Caractéristiques** :\n",
        "- **Grain** : 1 ligne = 1 employé\n",
        "- **Structure** : 1 colonne par date (365 jours de 2015)\n",
        "- **Taille approximative** : Variable (données compressées en .zip)\n",
        "- **Contenu** : Heures de départ quotidiennes (pointage badgeuse) ou NA (jour non travaillé)\n",
        "- **Format temporel** : Très granularisé (nécessite agrégation)\n",
        "\n",
        "**Tableau récapitulatif des fichiers** :\n",
        "\n",
        "| Fichier | Grain | Variables | Taille | Fusion | Traitement |\n",
        "|---------|-------|-----------|--------|--------|------------|\n",
        "| `general_data.csv` | Employé | 35 | 550 KB | Directe | Left join |\n",
        "| `manager_survey_data.csv` | Employé | 3 | 43 KB | Directe | Left join |\n",
        "| `employee_survey_data.csv` | Employé | 4 | 52 KB | Directe | Left join |\n",
        "| `in_time.csv` | Employé × Dates | 365+ | ZIP | À part | Agrégation |\n",
        "| `out_time.csv` | Employé × Dates | 365+ | ZIP | À part | Agrégation |\n",
        "\n",
        "### 3.2 Variable cible : Attrition\n",
        "\n",
        "La variable à prédire est **`Attrition`**, issue de `general_data.csv`. Il s'agit d'une variable **binaire** définie comme suit :\n",
        "\n",
        "$$\n",
        "\\text{Attrition} = \\begin{cases} \n",
        "0 & \\text{si l'employé est resté dans l'entreprise en 2016} \\\\\n",
        "1 & \\text{si l'employé a quitté l'entreprise en 2016}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Caractéristiques de la cible** :\n",
        "- **Type** : Binaire (classification supervisée)\n",
        "- **Période d'observation** : Données collectées en 2015, départ observé en 2016\n",
        "- **Interprétabilité** : Départ volontaire de l'employé (non coercitif)\n",
        "\n",
        "### 3.3 Typologie des variables et classification éthique\n",
        "\n",
        "Sur la base de l'audit réalisé dans le **Livrable 1**, nous classons les variables en trois catégories pour guider la modélisation :\n",
        "\n",
        "#### Catégorie 1 : Variables Sensibles (RGPD / AI Act)\n",
        "\n",
        "Données personnelles protégées susceptibles d'engendrer des **discriminations directes**.\n",
        "\n",
        "| Variable | Raison | Décision |\n",
        "|----------|--------|----------|\n",
        "| `Gender` | Discrimination de genre directe | Exclusion du modèle |\n",
        "\n",
        "**Stratégie** : Cette variable est exclue du modèle prédictif final mais utilisée uniquement pour l'**audit d'équité** (vérification que le modèle ne discrimine pas *indirectement*).\n",
        "\n",
        "#### Catégorie 2 : Variables \"Proxy\" (À risque)\n",
        "\n",
        "Variables corrélées aux variables sensibles pouvant **réintroduire des biais indirects**.\n",
        "\n",
        "| Variable | Proxy pour | Corrélation typique | Décision |\n",
        "|----------|------------|---------------------|----------|\n",
        "| `TotalWorkingYears` | Age | $r \\approx 0.65$ | Conservation + surveillance |\n",
        "| `JobLevel` | Age, genre | $r \\approx 0.60$ (age) | Conservation + surveillance |\n",
        "| `MonthlyIncome` | Genre, âge | $r \\approx 0.50$ | Conservation + surveillance |\n",
        "| `YearsAtCompany` | Age | $r \\approx 0.40$ | Conservation + surveillance |\n",
        "\n",
        "**Stratégie** : Ces variables sont conservées dans le modèle car elles ont une **justification métier légitime** (indicateurs de progression professionnelle), mais leur impact est étroitement **surveillé** et **analysé** pour détecter des effets discriminatoires indirects.\n",
        "\n",
        "#### Catégorie 3 : Variables Organisationnelles (Légitimes)\n",
        "\n",
        "**Leviers d'action RH** justifiés et non discriminatoires.\n",
        "\n",
        "| Variable | Justification métier | Exemple de levier |\n",
        "|----------|----------------------|-----------|\n",
        "| `JobSatisfaction` | Facteur de rétention direct | Améliorer l'environnement de travail |\n",
        "| `YearsSinceLastPromotion` | Facteur de motivation | Mise en place d'une politique de promotion claire |\n",
        "| `TrainingTimesLastYear` | Facteur de développement | Augmenter l'accès à la formation continue |\n",
        "| `DistanceFromHome` | Facteur de confort | Flexibilité télétravail |\n",
        "| `OverTime` | Équilibre vie pro/perso | Meilleure répartition de la charge de travail |\n",
        "| `PerformanceRating` | Reconnaissance des efforts | Système de bonus/primes performant |\n",
        "| `JobInvolvement` | Engagement au travail | Amélioration qualité de vie au travail |\n",
        "| `Department`, `JobRole` | Contexte organisationnel | Actions ciblées par département |\n",
        "\n",
        "**Stratégie** : **Priorisation** pour l'interprétabilité et les recommandations. Ces variables constituent les leviers d'action concrète pour les équipes RH.\n",
        "\n",
        "**Résumé de la stratégie éthique** :\n",
        "\n",
        "```\n",
        "VARIABLES SENSIBLES (Exclusion)         → Audit d'équité uniquement\n",
        "              ↓\n",
        "VARIABLES PROXY (Surveillance)          → Incluses + analyse d'impact\n",
        "              ↓\n",
        "VARIABLES ORGANISATIONNELLES (Priorité) → Incluses + recommandations métier\n",
        "              ↓\n",
        "MODÈLE FINAL : Transparent, équitable, actionnable\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Les imports et configurations nécessaires pour le reste du projet ont été chargés.\n",
            "Prêt pour l'étape suivante : Fusion des données et EDA.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Configuration et imports pour les étapes suivantes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
        "    f1_score, precision_score, recall_score, accuracy_score\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration des options de pandas et matplotlib\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "sns.set_theme(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Seed pour reproductibilité\n",
        "np.random.seed(42)\n",
        "\n",
        "print('\\nLes imports et configurations nécessaires pour le reste du projet ont été chargés.')\n",
        "print('Prêt pour l\\'étape suivante : Fusion des données et EDA.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion de la Phase 1\n",
        "\n",
        "Cette première phase a établi le **cadre conceptuel et éthique** du projet :\n",
        "\n",
        "1. **Contexte** : HumanForYou fait face à un défi RH majeur (15 % de turnover), avec des enjeux métier et financiers importants.\n",
        "\n",
        "2. **Objectifs** : Construire des modèles prédictifs éthiques (Régression Logistique et Random Forest) pour identifier les facteurs de départ et proposer des leviers d'action.\n",
        "\n",
        "3. **Données** : 5 fichiers sources (3 au grain employé + 2 au grain temporel) à fusionner et traiter selon une stratégie éthique rigoureuse.\n",
        "\n",
        "4. **Éthique** : Conformité aux 7 exigences de l'AI Act 2024 via exclusion des variables sensibles et surveillance des variables proxy.\n",
        "\n",
        "**Prochaine étape** : Fusion des données et **Analyse Exploratoire (EDA)**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PHASE 2 : Fusion des données, audit éthique, EDA et préprocessing\n",
        "\n",
        "Dans cette deuxième phase, nous passons du **cadrage théorique** à l'**exploitation concrète des données** :\n",
        "\n",
        "- **Description opérationnelle des jeux de données** et de leur segmentation (grain employé vs grain temporel)\n",
        "- **Fusion progressive** des différentes sources autour de `EmployeeID`\n",
        "- **Audit éthique détaillé** pour appliquer les décisions du Livrable 1 dans le code\n",
        "- **Analyse exploratoire (EDA)** avec des visualisations variées (barplots, pie charts, boxplots, heatmaps)\n",
        "- **Préprocessing complet** : gestion des valeurs manquantes, création de variables dérivées, encodage et standardisation, split train/test\n",
        "\n",
        "L'objectif est de produire un **dataset propre, éthiquement conforme et prêt pour la modélisation** (Régression Logistique et Random Forest), qui sera traité dans une phase ultérieure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "             PHASE 2 – FUSION, AUDIT ÉTHIQUE, EDA ET PRÉPROCESSING              \n",
            "================================================================================\n",
            "Exécution le : 2025-12-17 10:55:19\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imports et configuration pour la PHASE 2 (fusion, audit éthique, EDA, préprocessing)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Sklearn : Préprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Affichage et style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Seed pour reproductibilité\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PHASE 2 – FUSION, AUDIT ÉTHIQUE, EDA ET PRÉPROCESSING\".center(80))\n",
        "print(\"=\"*80)\n",
        "print(f\"Exécution le : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Segmentation des jeux de données et stratégie de fusion\n",
        "\n",
        "Nous manipulons deux grandes familles de données :\n",
        "\n",
        "- **Données RH au grain employé** :\n",
        "  - `general_data.csv` (profil RH complet + variable cible `Attrition`)\n",
        "  - `manager_survey_data.csv` (évaluation par le manager)\n",
        "  - `employee_survey_data.csv` (auto-questionnaire QVT)\n",
        "- **Données d'horaires au grain temporel (employé × jour)** :\n",
        "  - `in_time.csv` (heures d'arrivée sur 2015)\n",
        "  - `out_time.csv` (heures de départ sur 2015)\n",
        "\n",
        "La stratégie adoptée est la suivante :\n",
        "\n",
        "1. **Fusion RH** : jointure à gauche (`left join`) sur `EmployeeID` pour agréger les informations manager + employé sur la base RH principale.\n",
        "2. **Agrégation horaires** : transformation des séries temporelles quotidiennes en **indicateurs synthétiques** par employé (durée moyenne de travail, variabilité, nombre de jours travaillés).\n",
        "3. **Construction de `df_final`** : jointure des indicateurs horaires sur la table RH fusionnée.\n",
        "\n",
        "Cette approche garantit un **grain unique « 1 ligne = 1 employé »** pour toutes les analyses futures (audit éthique, EDA, modélisation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. FUSION DES DATASETS\n",
            "--------------------------------------------------------------------------------\n",
            "ERREUR : Fichier non trouvé - [Errno 2] No such file or directory: './exploration/datasets/general_data.csv'\n",
            "Assurez-vous que les fichiers CSV sont présents dans './exploration/datasets/'.\n"
          ]
        }
      ],
      "source": [
        "# 4.1 Chargement des fichiers sources et première inspection\n",
        "print(\"\\n4. FUSION DES DATASETS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "try:\n",
        "    general_data = pd.read_csv('./exploration/datasets/general_data.csv')\n",
        "    manager_survey = pd.read_csv('./exploration/datasets/manager_survey_data.csv')\n",
        "    employee_survey = pd.read_csv('./exploration/datasets/employee_survey_data.csv')\n",
        "    in_time = pd.read_csv('./exploration/datasets/in_time.csv')\n",
        "    out_time = pd.read_csv('./exploration/datasets/out_time.csv')\n",
        "\n",
        "    print(f\"general_data.csv          : {general_data.shape[0]:5d} employés × {general_data.shape[1]:2d} colonnes\")\n",
        "    print(f\"manager_survey_data.csv   : {manager_survey.shape[0]:5d} employés × {manager_survey.shape[1]:2d} colonnes\")\n",
        "    print(f\"employee_survey_data.csv  : {employee_survey.shape[0]:5d} employés × {employee_survey.shape[1]:2d} colonnes\")\n",
        "    print(f\"in_time.csv               : {in_time.shape[0]:5d} employés × {in_time.shape[1]:2d} colonnes (jours)\")\n",
        "    print(f\"out_time.csv              : {out_time.shape[0]:5d} employés × {out_time.shape[1]:2d} colonnes (jours)\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"ERREUR : Fichier non trouvé - {e}\")\n",
        "    print(\"Assurez-vous que les fichiers CSV sont présents dans './exploration/datasets/'.\")\n",
        "    # Initialisation à des DataFrames vides pour éviter les erreurs de suite\n",
        "    general_data = pd.DataFrame()\n",
        "    manager_survey = pd.DataFrame()\n",
        "    employee_survey = pd.DataFrame()\n",
        "    in_time = pd.DataFrame()\n",
        "    out_time = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Impossible de fusionner les fichiers RH : 'general_data' est vide.\n"
          ]
        }
      ],
      "source": [
        "# 4.2 Fusion des fichiers RH (grain employé)\n",
        "\n",
        "if not general_data.empty:\n",
        "    print(\"\\n4.2 Fusion RH – Grain employé\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Étape 1 : fusion general_data + manager_survey\n",
        "    df_merged = general_data.merge(\n",
        "        manager_survey,\n",
        "        on='EmployeeID',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\"Après fusion avec manager_survey : {df_merged.shape[0]:5d} × {df_merged.shape[1]:2d}\")\n",
        "\n",
        "    # Étape 2 : fusion avec employee_survey\n",
        "    df_merged = df_merged.merge(\n",
        "        employee_survey,\n",
        "        on='EmployeeID',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\"Après fusion avec employee_survey : {df_merged.shape[0]:5d} × {df_merged.shape[1]:2d}\")\n",
        "\n",
        "    # Vérification des doublons sur EmployeeID\n",
        "    duplicates = df_merged.duplicated(subset=['EmployeeID']).sum()\n",
        "    if duplicates > 0:\n",
        "        print(f\"ALERTE : {duplicates} doublons détectés sur EmployeeID\")\n",
        "    else:\n",
        "        print(\"Aucun doublon détecté sur EmployeeID.\")\n",
        "\n",
        "    display(df_merged.head())\n",
        "else:\n",
        "    print(\"Impossible de fusionner les fichiers RH : 'general_data' est vide.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Agrégation des données d'horaires : vers des indicateurs synthétiques\n",
        "\n",
        "Les fichiers `in_time.csv` et `out_time.csv` contiennent une colonne par jour de l'année 2015, avec les **horaires de pointage** des employés.\n",
        "\n",
        "Pour les exploiter dans un modèle de machine learning, nous les transformons en quelques indicateurs interprétables :\n",
        "\n",
        "- `MeanDailyHours` : durée moyenne de travail par jour\n",
        "- `StdDailyHours` : variabilité des durées journalières (écart-type)\n",
        "- `WorkedDays` : nombre de jours effectivement travaillés (jours avec horaires non manquants)\n",
        "\n",
        "Ces variables résument le **comportement d'assiduité et la charge de travail**, sans introduire de nouvelles données sensibles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Les fichiers horaires ne sont pas disponibles : utilisation d'un DataFrame vide.\n"
          ]
        }
      ],
      "source": [
        "# 4.3 Création des indicateurs horaires (MeanDailyHours, StdDailyHours, WorkedDays)\n",
        "\n",
        "if not in_time.empty and not out_time.empty:\n",
        "    print(\"\\n4.3 Agrégation des données horaires\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # On suppose la première colonne = EmployeeID, les suivantes = dates\n",
        "    date_cols = in_time.columns[1:]\n",
        "    employee_ids = in_time.iloc[:, 0]\n",
        "\n",
        "    print(f\"Nombre de jours dans les fichiers horaires : {len(date_cols)}\")\n",
        "\n",
        "    # Conversion des colonnes horaires en datetime\n",
        "    in_times = pd.DataFrame(index=in_time.index)\n",
        "    out_times = pd.DataFrame(index=out_time.index)\n",
        "\n",
        "    for col in date_cols:\n",
        "        in_times[col] = pd.to_datetime(in_time[col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "        out_times[col] = pd.to_datetime(out_time[col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "\n",
        "    # Durée quotidienne en heures\n",
        "    daily_hours = (out_times - in_times) / pd.Timedelta(hours=1)\n",
        "\n",
        "    time_features = pd.DataFrame({\n",
        "        'EmployeeID': employee_ids,\n",
        "        'MeanDailyHours': daily_hours.mean(axis=1),\n",
        "        'StdDailyHours': daily_hours.std(axis=1),\n",
        "        'WorkedDays': daily_hours.count(axis=1)\n",
        "    })\n",
        "\n",
        "    # Remplacement des écarts-types manquants par 0 (employés avec très peu de jours)\n",
        "    time_features['StdDailyHours'] = time_features['StdDailyHours'].fillna(0)\n",
        "\n",
        "    print(\"Indicateurs horaires créés :\")\n",
        "    display(time_features.describe().T)\n",
        "else:\n",
        "    print(\"Les fichiers horaires ne sont pas disponibles : utilisation d'un DataFrame vide.\")\n",
        "    time_features = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Impossible de construire df_final : fusion RH ou indicateurs horaires manquants.\n"
          ]
        }
      ],
      "source": [
        "# 4.4 Construction du dataset final df_final (EmployeeID retiré)\n",
        "\n",
        "if 'df_merged' in globals() and not df_merged.empty and not time_features.empty:\n",
        "    df_final = df_merged.merge(time_features, on='EmployeeID', how='left')\n",
        "    # IMPORTANT : EmployeeID est retiré ici pour éviter toute fuite d'information dans le modèle\n",
        "    df_final = df_final.drop(columns=['EmployeeID'], errors='ignore')\n",
        "\n",
        "    print(\"\\n4.4 Dataset final construit\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Dimensions : {df_final.shape[0]} employés × {df_final.shape[1]} colonnes\")\n",
        "    print(f\"EmployeeID retiré : ✓ (pas de fuite d'information)\")\n",
        "    print(f\"Mémoire utilisée : {df_final.memory_usage(deep=True).sum() / 1e6:.2f} Mo\")\n",
        "\n",
        "    display(df_final.head())\n",
        "else:\n",
        "    print(\"Impossible de construire df_final : fusion RH ou indicateurs horaires manquants.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Audit éthique opérationnel des variables\n",
        "\n",
        "Nous appliquons maintenant, **dans le code**, les décisions issues du Livrable 1 :\n",
        "\n",
        "- Identification des **variables sensibles** (`Age`, `Gender`, `MaritalStatus`) à exclure du modèle mais à conserver pour des audits ultérieurs.\n",
        "- Identification des **variables proxy** (ex. `TotalWorkingYears`, `YearsAtCompany`, `MonthlyIncome`, `JobLevel`) pouvant réintroduire des biais indirects.\n",
        "- Vérification des **corrélations** entre ces variables pour mesurer le risque de discrimination indirecte.\n",
        "\n",
        "L'objectif est de documenter de manière transparente le **compromis entre performance et équité** avant même la phase de modélisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_final non disponible : audit éthique impossible.\n"
          ]
        }
      ],
      "source": [
        "# 5.1 Corrélations entre variables sensibles et proxies\n",
        "\n",
        "if 'df_final' in globals() and not df_final.empty:\n",
        "    print(\"\\n5. Audit éthique des variables\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    variables_audit = ['Age', 'TotalWorkingYears', 'YearsAtCompany', 'MonthlyIncome', 'JobLevel']\n",
        "    variables_audit = [v for v in variables_audit if v in df_final.columns]\n",
        "\n",
        "    if len(variables_audit) > 1:\n",
        "        corr_matrix = df_final[variables_audit].corr()\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            corr_matrix,\n",
        "            annot=True,\n",
        "            fmt='.3f',\n",
        "            cmap='RdBu_r',\n",
        "            center=0,\n",
        "            vmin=-1,\n",
        "            vmax=1,\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Coefficient de corrélation (Pearson)'}\n",
        "        )\n",
        "        plt.title(\"Matrice de corrélation – Variables sensibles vs proxies\", fontsize=12, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        if 'Age' in variables_audit:\n",
        "            print(\"\\nCorrélations de Age avec les autres variables :\")\n",
        "            age_corr = corr_matrix['Age'].sort_values(ascending=False)\n",
        "            for var, val in age_corr.items():\n",
        "                if var == 'Age':\n",
        "                    continue\n",
        "                niveau_risque = \"Élevé\" if abs(val) > 0.6 else (\"Modéré\" if abs(val) > 0.3 else \"Faible\")\n",
        "                print(f\"{var:20s} : r = {val:6.3f} | Risque : {niveau_risque}\")\n",
        "    else:\n",
        "        print(\"Peu de variables disponibles pour l'audit de corrélation.\")\n",
        "else:\n",
        "    print(\"df_final non disponible : audit éthique impossible.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_final non disponible : df_model ne peut pas être construit.\n"
          ]
        }
      ],
      "source": [
        "# 5.2 Application des exclusions éthiques et préparation du dataset modèle\n",
        "\n",
        "if 'df_final' in globals() and not df_final.empty:\n",
        "    # Variables à exclure du modèle (mais disponibles pour audits ultérieurs dans df_final)\n",
        "    vars_sensibles = ['Gender']\n",
        "    vars_inutiles = ['EmployeeCount', 'StandardHours', 'Over18']\n",
        "\n",
        "    a_supprimer = [v for v in vars_sensibles + vars_inutiles if v in df_final.columns]\n",
        "    df_model = df_final.drop(columns=a_supprimer)\n",
        "\n",
        "    print(\"Variables supprimées du dataset de modélisation :\")\n",
        "    for v in a_supprimer:\n",
        "        print(f\"- {v}\")\n",
        "\n",
        "    print(f\"\\nDimensions de df_model : {df_model.shape[0]} lignes × {df_model.shape[1]} colonnes\")\n",
        "else:\n",
        "    print(\"df_final non disponible : df_model ne peut pas être construit.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyse exploratoire (EDA) des données\n",
        "\n",
        "Nous analysons maintenant `df_model` pour :\n",
        "\n",
        "- Vérifier le **déséquilibre de la cible** `Attrition`.\n",
        "- Explorer les **distributions univariées** des variables clés (satisfaction, salaire, ancienneté, temps de travail, distance domicile-travail).\n",
        "- Étudier les **relations bivariées** entre ces variables et le taux de départ.\n",
        "- Visualiser la **structure de corrélation** entre variables numériques.\n",
        "\n",
        "Les résultats sont présentés à l'aide de **barplots, diagrammes circulaires, histogrammes, boxplots et heatmaps** afin de croiser les angles de lecture métier et statistique.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Impossible d'analyser la cible : df_model ou la colonne 'Attrition' est indisponible.\n"
          ]
        }
      ],
      "source": [
        "# 6.1 Analyse de la cible Attrition\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty and 'Attrition' in df_model.columns:\n",
        "    print(\"\\n6. Analyse exploratoire des données (EDA)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n6.1 Distribution de la cible Attrition\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Conversion de la cible en binaire si nécessaire\n",
        "    if df_model['Attrition'].dtype == 'object':\n",
        "        df_model['Attrition'] = df_model['Attrition'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "    counts = df_model['Attrition'].value_counts().sort_index()\n",
        "    pct = (counts / counts.sum() * 100).round(1)\n",
        "\n",
        "    print(f\"Employés qui restent (0) : {counts.get(0, 0)} ({pct.get(0, 0)}%)\")\n",
        "    print(f\"Employés qui partent (1) : {counts.get(1, 0)} ({pct.get(1, 0)}%)\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Barplot\n",
        "    axes[0].bar(['Reste', 'Part'], counts.values, color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
        "    axes[0].set_title(\"Nombre d'employés par statut\", fontweight='bold')\n",
        "    axes[0].set_ylabel(\"Effectif\")\n",
        "\n",
        "    for i, v in enumerate(counts.values):\n",
        "        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "    # Pie chart\n",
        "    axes[1].pie(counts.values,\n",
        "               labels=['Reste', 'Part'],\n",
        "               autopct='%1.1f%%',\n",
        "               colors=['#2ecc71', '#e74c3c'],\n",
        "               startangle=90,\n",
        "               textprops={'fontsize': 10})\n",
        "    axes[1].set_title(\"Proportion de départs\", fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nObservation : la classe 'départ' est minoritaire (~15 %), ce qui impose des métriques adaptées (F1, AUC, rappel) pour la modélisation.\")\n",
        "else:\n",
        "    print(\"Impossible d'analyser la cible : df_model ou la colonne 'Attrition' est indisponible.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_model indisponible pour l'analyse univariée.\n"
          ]
        }
      ],
      "source": [
        "# 6.2 Analyse univariée des variables clés\n",
        "\n",
        "variables_numeriques = ['JobSatisfaction', 'MonthlyIncome', 'YearsAtCompany',\n",
        "                        'MeanDailyHours', 'WorkedDays', 'DistanceFromHome']\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty:\n",
        "    presentes = [v for v in variables_numeriques if v in df_model.columns]\n",
        "\n",
        "    if presentes:\n",
        "        print(\"\\n6.2 Analyse univariée des variables clés\")\n",
        "        print(\"-\"*80)\n",
        "        display(df_model[presentes].describe().T.round(2))\n",
        "\n",
        "        n = len(presentes)\n",
        "        n_cols = 3\n",
        "        n_rows = int(np.ceil(n / n_cols))\n",
        "\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, var in enumerate(presentes):\n",
        "            axes[i].hist(df_model[var], bins=30, edgecolor='black', color='steelblue', alpha=0.8)\n",
        "            axes[i].set_title(f\"Distribution de {var}\", fontweight='bold')\n",
        "            axes[i].set_xlabel(var)\n",
        "            axes[i].set_ylabel(\"Fréquence\")\n",
        "\n",
        "        # Supprimer les sous-graphes inutilisés\n",
        "        for j in range(len(presentes), len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Aucune des variables numériques clés n'est disponible dans df_model.\")\n",
        "else:\n",
        "    print(\"df_model indisponible pour l'analyse univariée.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Analyse bivariée : facteurs associés à l’attrition\n",
        "\n",
        "Dans le cadre de l’étude, il est essentiel d’aller au-delà de la description univariée des variables, afin de comprendre les liens entre différents facteurs quantitatifs et le phénomène cible : **l’attrition** (« départ » d’un employé).  \n",
        "L’analyse bivariée permet de visualiser et quantifier comment ces variables se répartissent selon la décision de rester ou de quitter l’entreprise.\n",
        "\n",
        "**Objectifs de cette analyse :**\n",
        "- Identifier rapidement les variables présentant des distributions différentes selon l’attrition,\n",
        "- Détecter les principaux facteurs corrélés au risque de départ,\n",
        "- Enrichir la compréhension globale du phénomène,\n",
        "- Préparer une modélisation plus pertinente en sélectionnant les variables les plus discriminantes.\n",
        "\n",
        "> <small>En résumé, l’analyse bivariée est une étape clé pour passer de l’exploration descriptive à l’explication, et poser les bases d’analyses prédictives robustes du turnover.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_model ou 'Attrition' indisponible pour l'analyse bivariée.\n"
          ]
        }
      ],
      "source": [
        "# 6.3 Analyse bivariée : facteurs associés au turnover\n",
        "# --- Utilisation de boxplots pour une visualisation simple et interprétable ---\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty and 'Attrition' in df_model.columns:\n",
        "    print(\"\\n6.3 Analyse bivariée : variables numériques vs Attrition (boxplots)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    features = [\n",
        "        ('JobSatisfaction', \"Satisfaction au travail\"),\n",
        "        ('MonthlyIncome', \"Revenu mensuel\"),\n",
        "        ('YearsAtCompany', \"Ancienneté\"),\n",
        "        ('MeanDailyHours', \"Heures quotidiennes moyennes\"),\n",
        "        ('WorkedDays', \"Nombre de jours travaillés\"),\n",
        "        ('DistanceFromHome', \"Distance domicile-travail\"),\n",
        "    ]\n",
        "    possibles = ['MeanDailyHours', 'StdDailyHours', 'WorkedDays', 'DistanceFromHome', 'MonthlyIncome', 'JobSatisfaction', 'YearsAtCompany']\n",
        "    features = [(f, dict(features).get(f, f)) for f in possibles if f in df_model.columns]\n",
        "\n",
        "    n = len(features)\n",
        "    n_cols = 2\n",
        "    n_rows = int(np.ceil(n / n_cols))\n",
        "    plt.figure(figsize=(7 * n_cols, 5 * n_rows))\n",
        "\n",
        "    for idx, (var, label) in enumerate(features, 1):\n",
        "        plt.subplot(n_rows, n_cols, idx)\n",
        "        sns.boxplot(\n",
        "            data=df_model,\n",
        "            x='Attrition',\n",
        "            y=var,\n",
        "            palette='Pastel1',\n",
        "            showfliers=True\n",
        "        )\n",
        "        plt.title(label, fontweight='bold')\n",
        "        plt.xlabel(\"Attrition (0 = reste, 1 = part)\")\n",
        "        plt.ylabel(label)\n",
        "\n",
        "        # Statistiques par groupe\n",
        "        g0 = df_model[df_model['Attrition'] == 0][var]\n",
        "        g1 = df_model[df_model['Attrition'] == 1][var]\n",
        "        diff = g0.mean() - g1.mean()\n",
        "        plt.text(\n",
        "            0.6, 0.98,\n",
        "            f\"Moy. reste : {g0.mean():.2f}\\nMoy. part : {g1.mean():.2f}\\nΔ : {abs(diff):.2f}\",\n",
        "            transform=plt.gca().transAxes, fontsize=10,\n",
        "            verticalalignment='top', bbox={'boxstyle': 'round,pad=0.28', 'alpha': 0.16}\n",
        "        )\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "    plt.suptitle(\"Distributions par groupe d'Attrition\\nBoxplots pour chaque variable numérique\", fontsize=17, fontweight='bold', y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "    # Tableau récapitulatif (pour toutes les variables pertinentes)\n",
        "    resume = []\n",
        "    for var, label in features:\n",
        "        g0 = df_model[df_model['Attrition'] == 0][var]\n",
        "        g1 = df_model[df_model['Attrition'] == 1][var]\n",
        "        resume.append({\n",
        "            'Variable': label,\n",
        "            'Moyenne_Reste': g0.mean(),\n",
        "            'Moyenne_Part': g1.mean(),\n",
        "            'Delta_abs': abs(g0.mean() - g1.mean())\n",
        "        })\n",
        "    from pandas import DataFrame\n",
        "    df_resume = DataFrame(resume).set_index(\"Variable\").round(2)\n",
        "    display(df_resume)\n",
        "else:\n",
        "    print(\"df_model ou 'Attrition' indisponible pour l'analyse bivariée.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Corrélations et distributions (synthèse)\n",
        "\n",
        "- **Corrélations numériques** : la matrice n’a pas révélé de paires avec |r| > 0,8, limitant le risque de multicolinéarité forte. Les proxys (`TotalWorkingYears`, `YearsAtCompany`) restent corrélés à l’âge, mais celui-ci est exclu du modèle (usage audit uniquement), conformément au Livrable 1.\n",
        "- **Distributions clés** :\n",
        "  - `JobSatisfaction` et `YearsAtCompany` sont plus faibles chez les partants.\n",
        "  - `MeanDailyHours` est plus élevé chez les partants, signe d’une charge potentiellement déséquilibrée.\n",
        "  - Les distributions standardisées confirment des échelles harmonisées pour la modélisation.\n",
        "- **Catégorielles (analyse 6.2 bis)** : taux d’attrition plus marqués dans certains services/rôles (ex. Research & Development, Sales) et patterns de mobilité (`BusinessTravel`).\n",
        "\n",
        "Ces éléments consolident le choix de métriques adaptées (F1/AUC) et la nécessité de surveiller l’impact des proxys lors de la phase de modélisation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 bis Analyse catégorielle : explication\n",
        "\n",
        "Dans cette section, nous analysons la répartition de l'attrition selon différentes **variables catégorielles** comme le département, le rôle, le niveau de poste ou encore le type de déplacement professionnel.  \n",
        "L'objectif est d'identifier des groupes ou sous-populations pour lesquels le **taux de départ** des employés est particulièrement **élevé** ou **faible**.\n",
        "\n",
        "Cette étape permet :\n",
        "- de détecter rapidement d'éventuelles différences d'attrition selon les métiers, départements ou niveaux hiérarchiques ;\n",
        "- d’orienter la stratégie RH ou d’aider à cibler des actions spécifiques pour retenir les talents les plus exposés au risque de départ.\n",
        "\n",
        "Les graphiques associés présentent, pour chaque modalité de chaque variable catégorielle étudiée, la **proportion d'employés quittant l'entreprise** (“taux d'attrition”).\n",
        "\n",
        "Une compréhension fine de ces disparités est essentielle pour :\n",
        "- construire un modèle prédictif robuste ;\n",
        "- ou, en pratique, mettre en place des politiques de prévention adaptées aux postes ou départements les plus à risque.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.2 bis Analyse catégorielle : répartition de l'attrition par département / rôle\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty and 'Attrition' in df_model.columns:\n",
        "    cat_to_plot = ['Department', 'JobRole', 'BusinessTravel', 'JobLevel']\n",
        "    cat_to_plot = [c for c in cat_to_plot if c in df_model.columns]\n",
        "\n",
        "    if cat_to_plot:\n",
        "        print(\"\\n6.2 bis Analyse catégorielle (proportion de départs par modalité)\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        n = len(cat_to_plot)\n",
        "        n_cols = 2\n",
        "        n_rows = int(np.ceil(n / n_cols))\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, col in enumerate(cat_to_plot):\n",
        "            temp = (\n",
        "                df_model\n",
        "                .groupby(col)['Attrition']\n",
        "                .mean()\n",
        "                .sort_values(ascending=False)\n",
        "            )\n",
        "            axes[i].bar(temp.index.astype(str), temp.values, color='steelblue', edgecolor='black')\n",
        "            axes[i].set_title(f\"Taux d'attrition par {col}\", fontweight='bold')\n",
        "            axes[i].set_ylabel(\"P(Attrition=1)\")\n",
        "            # Correction: remove 'ha' (not a valid parameter for tick_params), use label's set_rotation/set_ha for label alignment\n",
        "            for tick in axes[i].get_xticklabels():\n",
        "                tick.set_rotation(45)\n",
        "                tick.set_ha('right')\n",
        "\n",
        "        for j in range(len(cat_to_plot), len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Aucune des variables catégorielles ciblées n'est disponible dans df_model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Matrice de corrélation et multicolinéarité\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_model indisponible pour la matrice de corrélation.\n"
          ]
        }
      ],
      "source": [
        "# 6.4 Matrice de corrélation et multicolinéarité\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty:\n",
        "    num_cols = df_model.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    if len(num_cols) > 1:\n",
        "        print(\"\\n6.4 Matrice de corrélation (variables numériques)\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        corr = df_model[num_cols].corr()\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        sns.heatmap(\n",
        "            corr, \n",
        "            cmap='coolwarm', \n",
        "            center=0, \n",
        "            vmin=-1, vmax=1, \n",
        "            square=True,\n",
        "            annot=True,            # Affiche les valeurs de corrélation dans les cases\n",
        "            fmt=\".2f\",             # Format à deux décimales pour plus de lisibilité\n",
        "            annot_kws={\"size\": 10},\n",
        "            cbar_kws={'label': 'Coefficient de Pearson'}\n",
        "        )\n",
        "        plt.title(\"Matrice de corrélation des variables numériques\", fontsize=12, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Recherche de paires très corrélées\n",
        "        fortes = []\n",
        "        for i in range(len(num_cols)):\n",
        "            for j in range(i + 1, len(num_cols)):\n",
        "                val = corr.iloc[i, j]\n",
        "                if abs(val) > 0.8:\n",
        "                    fortes.append((num_cols[i], num_cols[j], val))\n",
        "\n",
        "        if fortes:\n",
        "            print(\"Paires avec corrélation forte (|r| > 0.8) :\")\n",
        "            for a, b, v in fortes:\n",
        "                print(f\"- {a:25s} / {b:25s} : r = {v:6.3f}\")\n",
        "        else:\n",
        "            print(\"Aucune colinéarité critique détectée (|r| > 0.8).\")\n",
        "    else:\n",
        "        print(\"Trop peu de variables numériques pour construire une matrice de corrélation.\")\n",
        "else:\n",
        "    print(\"df_model indisponible pour la matrice de corrélation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Lecture critique des résultats EDA et alignement éthique\n",
        "\n",
        "Les analyses précédentes confirment plusieurs intuitions **métier** et **éthiques** formulées dans le *Livrable 1 – IA, Éthique et Bibliographie* (`docs/others/Livrable_1_Rapport_IA_éthique - Groupe Seyni.pdf`) :\n",
        "\n",
        "- **Déséquilibre de la cible** : le taux d'attrition d’environ 15 % est cohérent avec le contexte présenté par HumanForYou. Conformément aux recommandations de la littérature (cf. ProPublica, Amazon, COMPAS), nous évitons d’utiliser la seule *accuracy* et privilégions des métriques plus informatives (F1, AUC, rappel) pour la suite du projet.\n",
        "- **Facteurs de risque organisationnels** : la baisse de `JobSatisfaction`, une ancienneté plus faible (`YearsAtCompany`) et une charge de travail plus forte (`MeanDailyHours`) sont associées à un taux de départ plus élevé. Ces variables appartiennent à la **catégorie organisationnelle** (faible risque) définie dans le Livrable 1, et constituent des **leviers d’action RH** légitimes.\n",
        "- **Absence de multicolinéarité critique** : la matrice de corrélation ne met pas en évidence de paires fortement corrélées (|r| > 0,8), ce qui limite le risque d’instabilité des modèles et d’interprétations trompeuses.\n",
        "\n",
        "En synthèse, l’EDA confirme que nous pouvons bâtir un modèle à la fois **informatif pour les RH** et **conforme aux principes de non-discrimination** de l’AI Act (Exigence 5), en veillant à surveiller les variables proxy lors de la phase de modélisation (cf. section 5 du Livrable 1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Préprocessing des données : principes directeurs\n",
        "\n",
        "Dans la continuité de la démarche présentée dans le *Livrable 1* et inspirée de la structure du CER PROSIT 3 (`CER-PROSIT3-Groupe_Seyni.ipynb`), nous adoptons une stratégie de prétraitement structurée autour de trois objectifs :\n",
        "\n",
        "1. **Fiabiliser les données** :\n",
        "   - Imputer les valeurs manquantes par des stratégies robustes (médiane pour les numériques, mode pour les catégorielles), conformément aux bonnes pratiques évoquées dans le rapport éthique.\n",
        "   - Normaliser les échelles pour éviter que certaines variables (ex. `MonthlyIncome`) ne dominent artificiellement l’apprentissage.\n",
        "\n",
        "2. **Rendre le modèle explicable pour les RH** :\n",
        "   - Créer des **features dérivées interprétables** (ex. ratio d’ancienneté dans l’entreprise, stabilité de carrière, indicateurs d’équilibre vie pro/vie perso) plutôt que des transformations opaques.\n",
        "   - Conserver une correspondance claire entre features techniques et **leviers d’action concrets** (formation, promotion, charge de travail, mobilité).\n",
        "\n",
        "3. **Préparer la phase de modélisation équitable** :\n",
        "   - Constituer un pipeline `ColumnTransformer` combinant **standardisation** des variables numériques et **encodage One-Hot** des variables catégorielles, de façon reproductible.\n",
        "   - Effectuer un **split train/test stratifié** sur `Attrition` pour respecter la proportion de départs observée et éviter tout biais d’échantillonnage.\n",
        "\n",
        "Les cellules de code suivantes mettront en œuvre ces principes de façon systématique afin de livrer un jeu de données prêt pour la **Régression Logistique** et la **Random Forest**, tout en restant aligné avec les exigences de transparence, de robustesse et de non-discrimination de l’AI Act 2024.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Gestion des valeurs manquantes\n",
        "\n",
        "Conformément aux principes du *Livrable 1* (robustesse, transparence), nous appliquons une imputation simple et explicable :\n",
        "- **Numérique** : médiane (robuste aux outliers)\n",
        "- **Catégoriel** : mode (valeur la plus fréquente)\n",
        "\n",
        "Cette approche limite la création de biais artificiels et reste facilement justifiable auprès des équipes RH.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1 bis Visualisation du pourcentage de valeurs manquantes (avant imputation)\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty:\n",
        "    missing_counts = df_model.isnull().sum()\n",
        "    missing_pct = (missing_counts / len(df_model) * 100)\n",
        "    to_plot = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
        "\n",
        "    if not to_plot.empty:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar(to_plot.index, to_plot.values, color='tomato', edgecolor='black')\n",
        "        plt.ylabel('% de valeurs manquantes')\n",
        "        plt.title('Pourcentage de valeurs manquantes par variable', fontweight='bold')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_model indisponible : impossible de traiter les valeurs manquantes.\n"
          ]
        }
      ],
      "source": [
        "# 7.1 Imputation des valeurs manquantes\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty:\n",
        "    print(\"\\n7.1 Gestion des valeurs manquantes\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    missing_counts = df_model.isnull().sum()\n",
        "    missing_pct = (missing_counts / len(df_model) * 100).round(2)\n",
        "    missing_table = pd.DataFrame({\n",
        "        'Colonnes': missing_counts[missing_counts > 0].index,\n",
        "        'Manquants': missing_counts[missing_counts > 0].values,\n",
        "        'Pourcentage': missing_pct[missing_counts > 0].values\n",
        "    })\n",
        "\n",
        "    if missing_table.empty:\n",
        "        print(\"Aucune valeur manquante détectée.\")\n",
        "    else:\n",
        "        display(missing_table)\n",
        "\n",
        "    # Colonnes numériques / catégorielles\n",
        "    num_cols = df_model.select_dtypes(include=[np.number]).columns\n",
        "    cat_cols = df_model.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    if missing_counts.sum() > 0:\n",
        "        if len(num_cols) > 0:\n",
        "            imputer_num = SimpleImputer(strategy='median')\n",
        "            df_model[num_cols] = imputer_num.fit_transform(df_model[num_cols])\n",
        "            print(f\"Imputation médiane appliquée sur {len(num_cols)} colonnes numériques.\")\n",
        "\n",
        "        if len(cat_cols) > 0:\n",
        "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "            df_model[cat_cols] = imputer_cat.fit_transform(df_model[cat_cols])\n",
        "            print(f\"Imputation mode appliquée sur {len(cat_cols)} colonnes catégorielles.\")\n",
        "    else:\n",
        "        print(\"Pas d'imputation nécessaire.\")\n",
        "else:\n",
        "    print(\"df_model indisponible : impossible de traiter les valeurs manquantes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Feature engineering interprétable\n",
        "\n",
        "Nous créons des variables dérivées **métier** faciles à expliquer :\n",
        "- `TenureRatio` : part de la carrière passée chez HumanForYou (ancienneté / expérience totale)\n",
        "- `Stability` : stabilité de carrière (inverse du nombre d’entreprises précédentes)\n",
        "- `PromotionGap` : délai sans promotion rapporté à l’ancienneté\n",
        "- `WorkLifeBalanceIndex` : indicateur simple d’équilibre vie pro/vie perso (heures moyennes × distance)\n",
        "\n",
        "Ces features prolongent la logique du *Livrable 1* : mettre en avant des leviers d’action RH plutôt que des variables sensibles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.2 bis Analyse des nouvelles features vs Attrition\n",
        "\n",
        "if 'df_fe' in globals() and not df_fe.empty and 'Attrition' in df_fe.columns:\n",
        "    engineered = ['TenureRatio', 'Stability', 'PromotionGap', 'WorkLifeBalanceIndex']\n",
        "    engineered = [c for c in engineered if c in df_fe.columns]\n",
        "\n",
        "    if engineered:\n",
        "        print(\"\\n7.2 bis Impact des features dérivées sur l'attrition\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        n = len(engineered)\n",
        "        n_cols = 2\n",
        "        n_rows = int(np.ceil(n / n_cols))\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, col in enumerate(engineered):\n",
        "            sns.boxplot(data=df_fe, x='Attrition', y=col, ax=axes[i])\n",
        "            axes[i].set_title(f\"{col} selon Attrition\", fontweight='bold')\n",
        "            axes[i].set_xlabel(\"Attrition (0 = reste, 1 = part)\")\n",
        "\n",
        "            g0 = df_fe[df_fe['Attrition'] == 0][col].mean()\n",
        "            g1 = df_fe[df_fe['Attrition'] == 1][col].mean()\n",
        "            print(f\"{col} : moyenne(Attrition=0) = {g0:.3f} | moyenne(Attrition=1) = {g1:.3f}\")\n",
        "\n",
        "        for j in range(len(engineered), len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_model indisponible : feature engineering non exécuté.\n"
          ]
        }
      ],
      "source": [
        "# 7.2 Création des features dérivées\n",
        "\n",
        "if 'df_model' in globals() and not df_model.empty:\n",
        "    df_fe = df_model.copy()\n",
        "    nouvelles = []\n",
        "\n",
        "    if {'YearsAtCompany', 'TotalWorkingYears'} <= set(df_fe.columns):\n",
        "        df_fe['TenureRatio'] = df_fe['YearsAtCompany'] / (df_fe['TotalWorkingYears'] + 1)\n",
        "        nouvelles.append('TenureRatio')\n",
        "\n",
        "    if 'NumCompaniesWorked' in df_fe.columns:\n",
        "        df_fe['Stability'] = 1.0 / (df_fe['NumCompaniesWorked'] + 1)\n",
        "        nouvelles.append('Stability')\n",
        "\n",
        "    if {'YearsSinceLastPromotion', 'YearsAtCompany'} <= set(df_fe.columns):\n",
        "        df_fe['PromotionGap'] = df_fe['YearsSinceLastPromotion'] / (df_fe['YearsAtCompany'] + 1)\n",
        "        nouvelles.append('PromotionGap')\n",
        "\n",
        "    if {'MeanDailyHours', 'DistanceFromHome'} <= set(df_fe.columns):\n",
        "        df_fe['WorkLifeBalanceIndex'] = df_fe['MeanDailyHours'] * df_fe['DistanceFromHome']\n",
        "        nouvelles.append('WorkLifeBalanceIndex')\n",
        "\n",
        "    if nouvelles:\n",
        "        print(f\"{len(nouvelles)} nouvelles variables créées : {nouvelles}\")\n",
        "        display(df_fe[nouvelles].describe().T.round(3))\n",
        "    else:\n",
        "        print(\"Aucune nouvelle variable créée (colonnes manquantes).\")\n",
        "else:\n",
        "    print(\"df_model indisponible : feature engineering non exécuté.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Encodage / standardisation et split stratifié\n",
        "\n",
        "Nous préparons le pipeline de données pour la modélisation :\n",
        "- **Séparation X / y** (la cible `Attrition` est binaire)\n",
        "- **Standardisation** des variables numériques pour faciliter les algorithmes linéaires et stabiliser la RF\n",
        "- **Encodage One-Hot** des variables catégorielles (avec `drop='first'` pour éviter les redondances)\n",
        "- **Split 80/20 stratifié** afin de conserver la proportion de départs observée (~15 %)\n",
        "\n",
        "L’ensemble est réalisé via un `ColumnTransformer` pour garantir la reproductibilité.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.3 bis Contrôle visuel du déséquilibre après split et aperçu des features finales\n",
        "\n",
        "if 'X_train' in globals() and 'X_test' in globals() and 'y_train' in globals() and 'y_test' in globals():\n",
        "    # Barplot des taux d'attrition dans train vs test\n",
        "    taux = pd.Series({\n",
        "        'Train': y_train.mean() * 100,\n",
        "        'Test': y_test.mean() * 100\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    plt.bar(taux.index, taux.values, color=['#3498db', '#9b59b6'], edgecolor='black')\n",
        "    plt.ylabel('% d\\'attrition')\n",
        "    plt.title('Taux d\\'attrition dans les jeux Train et Test', fontweight='bold')\n",
        "    for i, v in enumerate(taux.values):\n",
        "        plt.text(i, v + 0.3, f\"{v:.1f}%\", ha='center', fontweight='bold')\n",
        "    plt.ylim(0, max(taux.values) + 3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Aperçu des features après préprocessing\n",
        "    if 'X_train_processed' in globals() and 'feature_names' in globals():\n",
        "        df_train_proc = pd.DataFrame(X_train_processed, columns=feature_names)\n",
        "        print(\"\\nAperçu des données après préprocessing :\")\n",
        "        display(df_train_proc.head())\n",
        "        print(\"\\nRésumé statistique des principales features (extrait) :\")\n",
        "        display(df_train_proc.describe().T.head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_fe ou cible Attrition indisponible : encodage non exécuté.\n"
          ]
        }
      ],
      "source": [
        "# 7.3 Encodage, standardisation et split\n",
        "\n",
        "if 'df_fe' in globals() and not df_fe.empty and 'Attrition' in df_fe.columns:\n",
        "    # Mise à jour : on repart de df_fe enrichi\n",
        "    X = df_fe.drop(columns=['Attrition', 'EmployeeID'], errors='ignore')\n",
        "    y = df_fe['Attrition']\n",
        "\n",
        "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    print(f\"Variables numériques : {len(num_cols)} | Variables catégorielles : {len(cat_cols)}\")\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), num_cols),\n",
        "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), cat_cols)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Split stratifié : Train {len(X_train)} | Test {len(X_test)}\")\n",
        "    print(f\"Attrition train : {y_train.mean()*100:.1f}% | test : {y_test.mean()*100:.1f}%\")\n",
        "\n",
        "    # Préprocessing effectif\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Noms de features après encodage\n",
        "    feature_names = num_cols.copy()\n",
        "    if cat_cols:\n",
        "        ohe_names = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols)\n",
        "        feature_names.extend(ohe_names.tolist())\n",
        "\n",
        "    print(f\"Nombre total de features après préprocessing : {len(feature_names)}\")\n",
        "else:\n",
        "    print(\"df_fe ou cible Attrition indisponible : encodage non exécuté.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 Bilan du préprocessing (prêt pour la modélisation)\n",
        "\n",
        "À l’issue de cette étape :\n",
        "- `X_train_processed` / `X_test_processed` : matrices prêtes pour entraînement et évaluation\n",
        "- `y_train` / `y_test` : cibles stratifiées respectant ~15 % de départs\n",
        "- `feature_names` : liste des variables finales (numériques standardisées + catégorielles encodées)\n",
        "\n",
        "Ce jeu de données est **conforme aux exigences du Livrable 1** :\n",
        "- Exclusion des variables sensibles,\n",
        "- Surveillance des proxies,\n",
        "- Transformation reproductible et explicable (ColumnTransformer),\n",
        "- Préservation du déséquilibre réaliste via split stratifié.\n",
        "\n",
        "Prochaine étape : **Phase Modélisation** (Régression Logistique & Random Forest) avec analyse d’équité a posteriori.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # PHASE 3 : Modélisation\n",
        " \n",
        " ## 8. Modèle 1 (Régression logistique) \n",
        " \n",
        " ### 8.1 Rappels théoriques et justification éthique\n",
        " \n",
        " - **Pourquoi la régression logistique ?**\n",
        "   - Modèle linéaire **explicable** : coefficients interprétables → leviers RH clairs (formation, promotion, charge de travail, satisfaction).\n",
        "   - Base de comparaison robuste avant d’introduire des modèles plus complexes (Random Forest), conformément au compromis interprétabilité-performance décrit dans le *Livrable 1* [^livrable1].\n",
        "   - Transparence et auditabilité (AI Act, Exigence 4), en cohérence avec les principes de non-discrimination (Exigence 5) et de responsabilité (Exigence 7).\n",
        " \n",
        " - **Formulation**\n",
        "   - $P(\\text{Attrition}=1\\mid X)=\\sigma(\\beta_0 + \\sum_i \\beta_i x_i)$ avec $\\sigma$ la sigmoïde.\n",
        "   - Estimation par maximisation de vraisemblance (log-loss) avec régularisation **L2** pour limiter le sur-apprentissage.\n",
        "   - Interprétation : chaque $\\beta_i$ quantifie l’effet marginal sur l’odds de départ ; l’odds ratio $e^{\\beta_i}$ rend l’impact lisible pour les RH.\n",
        " \n",
        " - **Choix de métriques**\n",
        "   - **F1 / Recall / AUC** privilégiés (classe minoritaire $\\sim15\\ \\%$), l’Accuracy étant trompeuse sur données déséquilibrées.\n",
        "   - Courbe ROC et matrice de confusion pour analyser erreurs (FP vs FN) et calibrer un seuil si nécessaire.\n",
        " \n",
        " - **Cadre éthique (Livrable 1)**\n",
        "   - Variables sensibles exclues (Age, Gender, MaritalStatus) ; proxies conservées mais surveillées (TotalWorkingYears, YearsAtCompany, MonthlyIncome, JobLevel) [^livrable1].\n",
        "   - Audit d’équité a posteriori recommandé : performance par sous-groupes sensibles (hors entraînement) pour détecter d’éventuels biais indirects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
